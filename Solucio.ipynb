{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "209177da",
      "metadata": {
        "scrolled": true,
        "id": "209177da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os, glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip response.zip"
      ],
      "metadata": {
        "id": "UVHeivT4ywAa"
      },
      "id": "UVHeivT4ywAa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1d65f689",
      "metadata": {
        "id": "1d65f689"
      },
      "source": [
        "# Create Data Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4d523f51",
      "metadata": {
        "scrolled": true,
        "id": "4d523f51"
      },
      "outputs": [],
      "source": [
        "N_IMAGES_TRAIN = 20_000\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "9d22e812",
      "metadata": {
        "scrolled": true,
        "id": "9d22e812"
      },
      "outputs": [],
      "source": [
        "# First we define a custom dataset class to be able to create the DataLoader properly\n",
        "\n",
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, labels_file, img_dir, separator=',', img_type=\"png\",transform=None, max_images=None):\n",
        "        # As labels, we select the 'c' column of the dataset, which contains the number\n",
        "        # of circles of the image\n",
        "        aux = pd.read_csv(labels_file,sep=separator)\n",
        "        aux.drop(columns=aux.columns[0], axis=1,inplace=True)\n",
        "        aux.drop(columns=aux.columns[1:], axis=1, inplace=True)\n",
        "        self.img_labels = aux.to_numpy().squeeze()\n",
        "        if max_images is not None:\n",
        "            self.img_labels = self.img_labels[:max_images]\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.img_type = img_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # To get the images, we transform the index to a 5 long string (so 28 -> 00028)\n",
        "        # and we use that to read the image.\n",
        "        img_name = str(idx).zfill(5)+'.'+self.img_type\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = cv2.imread(img_path,  cv2.IMREAD_GRAYSCALE)\n",
        "        label = self.img_labels[idx,]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            image = image.float()\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        return (image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e97a42e8",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "e97a42e8",
        "outputId": "153afff7-978b-4ffc-f782-1f99606a8c20"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n"
          ]
        }
      ],
      "source": [
        "# Testing custom dataset is correctly constructed\n",
        "dummy_dataset = CustomImageDataset(labels_file='data/val/dades.csv', img_dir='data/val', separator=';')\n",
        "batch = next(iter(dummy_dataset))\n",
        "plt.imshow(batch[0], cmap='gray')\n",
        "plt.show()\n",
        "print(batch[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "eff0293c",
      "metadata": {
        "scrolled": true,
        "id": "eff0293c"
      },
      "outputs": [],
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((128,128))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = CustomImageDataset(labels_file='data/train/dades.csv', img_dir='data/train', separator=';', max_images=N_IMAGES_TRAIN, transform=transform)\n",
        "test_dataset = CustomImageDataset(labels_file='data/val/dades.csv', img_dir='data/val', separator=';', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, TRAIN_BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "edd8121f",
      "metadata": {
        "scrolled": true,
        "id": "edd8121f"
      },
      "outputs": [],
      "source": [
        "class FullyConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConvNet, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv3 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv4 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv5 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv6 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv7 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.conv8 = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.linear = nn.Linear(8*8, 32)\n",
        "        self.linear2 = nn.Linear(32, 1)\n",
        "        self.relu =nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.max_pool(x) # out = 64x64x2\n",
        "        x = self.conv2(x)\n",
        "        x = self.max_pool(x) # out = 32x32x4\n",
        "        x = self.conv3(x)\n",
        "        x = self.max_pool(x) # out = 16x16x8\n",
        "        x = self.conv4(x)\n",
        "        x = self.max_pool(x) # out = 8x8x16\n",
        "        x = self.conv5(x)    # out = 8x8x8\n",
        "        x = self.conv6(x)    # out = 8x8x4\n",
        "        x = self.conv7(x)    # out = 8x8x2\n",
        "        x = self.conv8(x)    # out = 8x8x1\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "6aaa6f05",
      "metadata": {
        "scrolled": true,
        "id": "6aaa6f05"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    loss_v = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "\n",
        "        loss = loss_func(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0 and verbose:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
        "        loss_v += loss.item()\n",
        "\n",
        "    loss_v /= len(train_loader.dataset)\n",
        "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
        " \n",
        "    return loss_v\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_func(output, target)\n",
        "            output = torch.round(output)\n",
        "            correct += output.eq(target.view_as(output)).sum().item()\n",
        " \n",
        "  \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e730055",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e730055",
        "outputId": "ce606c88-b461-4518-c2a2-dc5863257b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters  5218\n",
            "Train Epoch: 0 [0/20000 (0%)]\tLoss: 1.254091, Average: 0.009798\n",
            "Train Epoch: 0 [12800/20000 (64%)]\tLoss: 0.656801, Average: 0.005131\n",
            "\n",
            "Train set: Average loss: 0.0048\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0042, Accuracy: 1010/2000 (50%)\n",
            "\n",
            "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.386211, Average: 0.003017\n",
            "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.471504, Average: 0.003684\n",
            "\n",
            "Train set: Average loss: 0.0032\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0038, Accuracy: 1087/2000 (54%)\n",
            "\n",
            "Train Epoch: 2 [0/20000 (0%)]\tLoss: 0.332669, Average: 0.002599\n",
            "Train Epoch: 2 [12800/20000 (64%)]\tLoss: 0.442474, Average: 0.003457\n",
            "\n",
            "Train set: Average loss: 0.0030\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0035, Accuracy: 1132/2000 (57%)\n",
            "\n",
            "Train Epoch: 3 [0/20000 (0%)]\tLoss: 0.316489, Average: 0.002473\n",
            "Train Epoch: 3 [12800/20000 (64%)]\tLoss: 0.418270, Average: 0.003268\n",
            "\n",
            "Train set: Average loss: 0.0028\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0033, Accuracy: 1156/2000 (58%)\n",
            "\n",
            "Train Epoch: 4 [0/20000 (0%)]\tLoss: 0.295398, Average: 0.002308\n",
            "Train Epoch: 4 [12800/20000 (64%)]\tLoss: 0.390180, Average: 0.003048\n",
            "\n",
            "Train set: Average loss: 0.0026\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0030, Accuracy: 1231/2000 (62%)\n",
            "\n",
            "Train Epoch: 5 [0/20000 (0%)]\tLoss: 0.257088, Average: 0.002008\n",
            "Train Epoch: 5 [12800/20000 (64%)]\tLoss: 0.318788, Average: 0.002491\n",
            "\n",
            "Train set: Average loss: 0.0023\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 1264/2000 (63%)\n",
            "\n",
            "Train Epoch: 6 [0/20000 (0%)]\tLoss: 0.227399, Average: 0.001777\n",
            "Train Epoch: 6 [12800/20000 (64%)]\tLoss: 0.257696, Average: 0.002013\n",
            "\n",
            "Train set: Average loss: 0.0020\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 1355/2000 (68%)\n",
            "\n",
            "Train Epoch: 7 [0/20000 (0%)]\tLoss: 0.203990, Average: 0.001594\n",
            "Train Epoch: 7 [12800/20000 (64%)]\tLoss: 0.210546, Average: 0.001645\n",
            "\n",
            "Train set: Average loss: 0.0018\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 1390/2000 (70%)\n",
            "\n",
            "Train Epoch: 8 [0/20000 (0%)]\tLoss: 0.203954, Average: 0.001593\n",
            "Train Epoch: 8 [12800/20000 (64%)]\tLoss: 0.181493, Average: 0.001418\n",
            "\n",
            "Train set: Average loss: 0.0016\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 1488/2000 (74%)\n",
            "\n",
            "Train Epoch: 9 [0/20000 (0%)]\tLoss: 0.190612, Average: 0.001489\n",
            "Train Epoch: 9 [12800/20000 (64%)]\tLoss: 0.159043, Average: 0.001243\n",
            "\n",
            "Train set: Average loss: 0.0015\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0018, Accuracy: 1527/2000 (76%)\n",
            "\n",
            "Train Epoch: 10 [0/20000 (0%)]\tLoss: 0.181733, Average: 0.001420\n",
            "Train Epoch: 10 [12800/20000 (64%)]\tLoss: 0.147169, Average: 0.001150\n",
            "\n",
            "Train set: Average loss: 0.0014\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 1564/2000 (78%)\n",
            "\n",
            "Train Epoch: 11 [0/20000 (0%)]\tLoss: 0.172497, Average: 0.001348\n",
            "Train Epoch: 11 [12800/20000 (64%)]\tLoss: 0.138518, Average: 0.001082\n",
            "\n",
            "Train set: Average loss: 0.0013\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 1585/2000 (79%)\n",
            "\n",
            "Train Epoch: 12 [0/20000 (0%)]\tLoss: 0.163391, Average: 0.001276\n",
            "Train Epoch: 12 [12800/20000 (64%)]\tLoss: 0.128703, Average: 0.001005\n"
          ]
        }
      ],
      "source": [
        "use_cuda = True\n",
        "torch.manual_seed(33)\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "\n",
        "model = FullyConvNet().to(device)\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # !!!\n",
        "\n",
        "print(\"Parameters \", pytorch_total_params)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "# Guardam el valor de pèrdua promig de cada iteració (època)\n",
        "train_l = np.zeros((epochs))\n",
        "test_l = np.zeros((epochs))\n",
        "\n",
        "# Bucle d'entrenament\n",
        "for epoch in range(0, epochs):\n",
        "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
        "    test_l[epoch]  = test(model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}